RAG Mini Project – Document Question Answering
Overview
This mini project implements a Retrieval-Augmented Generation (RAG) system for document-based question answering using modern embedding models and vector databases. The system allows users to ask natural language questions over one or more PDF/TXT documents and receive context-grounded answers derived directly from the underlying content.
​

The pipeline covers the complete RAG workflow: document ingestion, text chunking, embedding generation, vector indexing, retrieval, and answer generation using a large language model.
​

Executive Summary
What is this Project?
This project builds a RAG application that transforms static documents into an interactive question-answering interface. Users can upload a document (e.g., course notes, reports, reference PDFs) and query it using natural language to quickly retrieve relevant information without manually scanning the entire file.
​

Project Goal
Convert raw PDFs/TXTs into a structured, searchable knowledge base.
​

Retrieve semantically relevant chunks for any user question.
​

Generate clear, concise answers that stay grounded in the original document context.
​

Main Steps (Key Implementation Stages)
Environment Setup – Install required libraries (e.g., sentence-transformers, faiss, pdfplumber, etc.).
​

Document Loading – Read PDF/TXT and extract raw text from the uploaded file.
​

Text Chunking – Split the long text into overlapping chunks suitable for semantic search.
​

Embedding Generation – Convert text chunks into dense vector representations using a Sentence Transformer model.
​

Vector Database Creation – Index embeddings using FAISS (or similar) to enable efficient similarity search.
​

Retriever Construction – Implement a retrieval function that finds top‑k relevant chunks for a given query.
​

RAG Answering Pipeline – Combine retrieved context and user query to produce an answer using an LLM or a stub generator.
​

Test Queries & Evaluation – Run multiple queries and qualitatively check answer relevance, grounding, and clarity.
​

Detailed Step-by-Step Explanation
Step 1: Environment Setup
Purpose:
Prepare the Python environment with all necessary packages for document processing, embeddings, and vector search.
​

Key Libraries:

pdfplumber or PyPDF2 – PDF parsing and text extraction.
​

sentence-transformers – Pretrained embedding models (e.g., all-MiniLM-L6-v2).
​

faiss-cpu – Vector similarity search and indexing.
​

Standard Python libraries – numpy, textwrap, typing, etc.
​

These libraries provide the infrastructure for reading documents, generating embeddings, storing them in a vector index, and performing fast similarity search.
​

Step 2: Document Loading
Purpose:
Load the source document(s) and extract a clean text corpus.
​

Supported Data Types:

PDF documents (via pdfplumber).
​

Plain text files (TXT), if provided.
​

Process:

Open the file path supplied in the notebook.
​

For PDFs: iterate over pages and concatenate page text into a single string.
​

For TXTs: read the entire file content directly.
​

The output of this step is a single long text string representing the full document content.
​

Step 3: Document Chunking Strategy
Purpose:
Split the long text into smaller, overlapping chunks that are easier to embed and retrieve.
​

Chunking Configuration (example used in the notebook):

Chunk size: ~800 characters per chunk.
​

Chunk overlap: ~200 characters between consecutive chunks.
​

Reasoning:

Chunk size is large enough to include meaningful context (paragraph-level information) but small enough to keep retrieval precise and efficient.
​

Overlap preserves continuity between chunks so that important information at boundaries is not lost.
​

Data Flow:

Raw Document Text → Sliding‑window split → Overlapping Chunks List → Ready for Embedding.
​

Step 4: Embedding Generation
Purpose:
Convert each text chunk into a numerical vector that captures its semantic meaning.
​

Embedding Model:

sentence-transformers/all-MiniLM-L6-v2.
​

Why this Model:

Lightweight and fast, suitable for local/Colab execution.
​

384‑dimensional embeddings, providing a good balance between expressiveness and efficiency.
​

Widely used baseline for semantic similarity tasks.
​

Process:

Load the SentenceTransformer model once.
​

Encode all chunks into a numpy array of shape [num_chunks, embedding_dim].
​

These embeddings are then used as keys for semantic search in the vector database.
​

Step 5: Vector Database Setup (FAISS)
Purpose:
Enable fast similarity search over the embeddings to retrieve the most relevant text chunks for any query.
​

Vector Store:

FAISS IndexFlatL2 (L2 distance on dense vectors).
​

Process:

Initialize an index with the correct embedding dimension.
​

Add all chunk embeddings into the index.
​

Benefits:

Efficient nearest‑neighbor search over hundreds or thousands of chunks.
​

Suitable for in‑memory experiments and small–medium datasets typical of course assignments.
​

Step 6: Retrieval Function
Purpose:
Given a user query, find the top‑k most semantically similar chunks from the document.
​

Mechanism:

Encode the query using the same embedding model.
​

Perform FAISS search to obtain distances and indices of the nearest chunks.
​

Map indices back to the original text chunks.
​

Key Parameter:

k (top‑k): Typically 3–5, balancing context coverage with prompt length.
​

The retrieval step is crucial for grounding the final answer in relevant portions of the document, reducing hallucination risk.
​

Step 7: RAG Answer Generation
Purpose:
Combine retrieved chunks and user query to generate a coherent, context‑aware answer.
​

Context Construction:

Concatenate top‑k chunks into a single context string with optional character‑limit.
​

Generation Logic:

In the template notebook, a stub function is provided that illustrates where an actual LLM call would occur.
​

For a full implementation, this stub can be replaced with:

OpenAI ChatCompletion,

A local LLM (e.g., via transformers),

Or any other API supported in the environment.
​

Prompt Concept:

“Given the following context from the document and the user’s question, provide a concise answer grounded only in the context.”
​

This step completes the RAG loop by producing an answer that is explicitly driven by retrieved document segments.
​

Step 8: Test Queries and Outputs
Purpose:
Validate that the pipeline works end‑to‑end and produces sensible answers.
​

Implementation:

Define at least three test questions covering:

Factual lookup,

Explanation/summary,

Possibly a slightly analytical question within the document’s scope.
​

For each query:

Retrieve relevant chunks,

Build context,

Generate an answer with the RAG function.
​

Expected Behavior:

Answers should be aligned with the document content and avoid external knowledge.
​

Retrieved context should be visibly related to the question if printed for debugging.
​

Core Technologies & Concepts
Retrieval-Augmented Generation (RAG)
Definition:
RAG combines document retrieval (via semantic search) and LLM generation to produce answers that are both fluent and grounded in specific source documents.

Advantages for this Project:

No fine‑tuning required; works with pretrained models.
​

Easy to update: add or replace documents without retraining the LLM.
​

Increased transparency: answers can be traced back to specific chunks.
​

Semantic Search vs Keyword Search
Keyword Search: Relies on exact word matches; may miss semantically related content using different phrasing.
​

Semantic Search: Uses embeddings to measure similarity in meaning, enabling robust retrieval even when wording differs.

In this project, semantic search via embeddings and FAISS is used to retrieve the most relevant chunks for each query.
​

Important Parameters and Their Impact
Parameter	Typical Value	Impact	Notes
Chunk size	~800 characters	Larger = more context, smaller = more precision	Tune based on document length
Chunk overlap	~200 characters	Preserves continuity at boundaries	~20–30% of chunk size works well
Top‑k retrieval	3–5 chunks	More chunks = more context but longer prompts	3–4 is often a good compromise
Embedding model	all‑MiniLM‑L6‑v2	Controls embedding quality & speed	Good default for student projects

Architecture Diagram (Textual)
High‑level architecture of the RAG pipeline:

Offline / Preprocessing Path
Documents → Text Extraction → Chunking → Embedding Model → FAISS Index.
​

Online / Query Path
User Query → Query Embedding → FAISS Search → Top‑k Chunks → Context Assembly → LLM (or stub generator) → Final Answer.
​

This separation highlights that document processing is done once, while query processing is dynamic at runtime.

Common Issues and Considerations
Irrelevant retrieval: Adjust chunk size, overlap, or top‑k; ensure the document is cleanly extracted.

Long responses or truncation: Limit context length or response tokens when using an API LLM.
​

Hallucination: Emphasize in the prompt that the model must use only provided context.
​

Conclusion and Key Takeaways
This mini project demonstrates how to convert static documents into an interactive QA system using a RAG architecture. By combining text chunking, semantic embeddings, vector search (FAISS), and LLM‑based generation, the system can deliver accurate, document‑grounded answers to user queries with minimal manual effort.

You can extend this project by:

Supporting multiple documents and metadata filters.

Replacing the stub generator with a production LLM API.

Adding evaluation scripts to automatically score relevance and faithfulness of answers.
